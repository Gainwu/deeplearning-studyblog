

# 2.5自动求导

- [2.5自动求导](#25自动求导)
  - [视频笔记（截图）](#视频笔记截图)
    - [1.反向传播](#1反向传播)
  - [2.5.1. 一个简单的例子](#251-一个简单的例子)
    - [1.函数求导](#1函数求导)
    - [2.示例：如何计算梯度](#2示例如何计算梯度)
  - [2.5.2. 非标量变量的反向传播](#252-非标量变量的反向传播)
  - [2.5.3. 分离计算](#253-分离计算)
  - [2.5.4. Python控制流的梯度计算](#254-python控制流的梯度计算)
  - [2.5.5. 练习](#255-练习)
    - [练习1](#练习1)
    - [练习2](#练习2)
    - [3.`在控制流的例子中，我们计算d关于a的导数，如果将变量a更改为随机向量或矩阵，会发生什么？`](#3在控制流的例子中我们计算d关于a的导数如果将变量a更改为随机向量或矩阵会发生什么)
    - [4.`重新设计一个求控制流梯度的例子，运行并分析结果。`](#4重新设计一个求控制流梯度的例子运行并分析结果)
    - [5.`使f (x) = sin(x)，绘制f(x)和df(x)/dx的图像，其中后者不使用f'(x)= cos(x)。`](#5使f-x--sinx绘制fx和dfxdx的图像其中后者不使用fx-cosx)


## 视频笔记（截图）

![image-20250103203831392](https://raw.githubusercontent.com/Gainwu/img_cloud/main/img/image-20250103203831392.png)

![image-20250103203925706](https://raw.githubusercontent.com/Gainwu/img_cloud/main/img/image-20250103203925706.png)

![image-20250103204004501](https://raw.githubusercontent.com/Gainwu/img_cloud/main/img/image-20250103204004501.png)

 ![image-20250103204048420](https://raw.githubusercontent.com/Gainwu/img_cloud/main/img/image-20250103204048420.png)

 ![image-20250103204150962](https://raw.githubusercontent.com/Gainwu/img_cloud/main/img/image-20250103204150962.png)

- 正向从右往左算，反向从左往右算，一般y是loss，网络梯度都是反向运算

![image-20250103204315967](https://raw.githubusercontent.com/Gainwu/img_cloud/main/img/image-20250103204315967.png)

![image-20250103204345137](https://raw.githubusercontent.com/Gainwu/img_cloud/main/img/image-20250103204345137.png)

 ![image-20250103204419139](https://raw.githubusercontent.com/Gainwu/img_cloud/main/img/image-20250103204419139.png)

- **内存复杂度：**消耗GPU资源的”祸源“

![image-20250103204628825](https://raw.githubusercontent.com/Gainwu/img_cloud/main/img/image-20250103204628825.png)

- **正向累计：**不需要储存结果，但计算一个梯度的结果需要扫一遍

### 1.反向传播

![img](https://raw.githubusercontent.com/Gainwu/img_cloud/main/img/1850883-20211011233337880-58600339.png)

## 2.5.1. 一个简单的例子

- 在我们计算y关于x的梯度之前，需要**分配一个地方来存储梯度**。 重要的是，我们不会在每次对一个参数求导时都分配新的内存。 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。

### 1.函数求导

$$
f(x)=2<x^T,x>
$$

> <x^T,x>为内积

```python
import torch

x = torch.arange(4.0)
x.requires_grad_(True)
x.grad
```

> ```python
> x.requires_grad_(True)
> ```
>
> - `requires_grad_(True)` 是一个 *in-place* 操作（即直接修改张量 `x`），它将张量 `x` 的 `requires_grad` 属性设置为 `True`。
> - 在 PyTorch 中，`requires_grad` 是一个标志，表示是否对该张量计算梯度。如果该标志为 `True`，则在执行张量的操作时，PyTorch 会自动追踪操作的计算图，并允许计算和反向传播梯度。
> - 默认情况下，创建的张量的 `requires_grad` 属性是 `False`，表示 PyTorch 不会计算这个张量的梯度。
> - 通过将 `x.requires_grad_(True)`，**启用了 `x` 对梯度的追踪，这样就可以对它进行梯度计算**了。
>
> ```python
> x.grad
> ```
>
> - `x.grad` 用来查看张量 `x` 的梯度。由于 `x` 的 `requires_grad` 已经被设置为 `True`，这意味着在某些操作（例如计算损失函数）之后，可以使用 `x.grad` 来访问 `x` 的梯度。
> - 没有对张量 `x` 执行任何操作来计算梯度，所以 `x.grad` 在此时应该返回 `None`，因为 PyTorch 尚未计算梯度。

```
y = 2 * torch.dot(x, x)
y.backward()
x.grad == 4 * x
```

> ```
> x = torch.arange(4.0, requires_grad=True)
> ```
>
> ```python
> y = 2 * torch.dot(x, x)
> ```
>
> - `torch.dot(x, x)` 是 `x` 的点积（内积），即 `x` 和 `x` 向量的逐元素相乘并求和。
>
>   对于 `x = [0, 1, 2, 3]`，点积的计算是：
>   $$
>   \text{dot}(x, x) = 0^2 + 1^2 + 2^2 + 3^2 = 0 + 1 + 4 + 9 = 14
>   $$
>
> - 然后我们将点积结果乘以 2，即：
>   $$
>   y = 2 \times 14 = 28
>   $$
>
> ```python
> y.backward()
> ```
>
> - 调用 `backward()` 会计算 `y` 相对于 `x` 的梯度。**由于 `y` 是标量，`y.backward()` 会对 `y` 相对于 `x` 进行反向传播，计算梯度并存储在 `x.grad` 中。**
>
> ```python
> x.grad
> ```
>
> - 函数 `y = 2 * torch.dot(x, x)` 的数学表达式：
>
> $$
> y = 2 \cdot (x_1^2 + x_2^2 + x_3^2 + x_4^2)
> $$
>
> - 现在我们需要计算 `y` 关于 `x` 的梯度。根据链式法则：
>
> $$
> \frac{\partial y}{\partial x_i} = \frac{\partial}{\partial x_i} \left( 2 \cdot (x_1^2 + x_2^2 + x_3^2 + x_4^2) \right)
> $$
>
> - 可以得到：
>
> $$
> \frac{\partial y}{\partial x_i} = 4x_i
> $$
>
> - 因此，梯度 `x.grad` 是：
>
> $$
> x.grad = [4 \cdot 0, 4 \cdot 1, 4 \cdot 2, 4 \cdot 3] = [0, 4, 8, 12]
> $$
>
> ```
> tensor([ 0.,  4.,  8., 12.])
> ```

### 2.示例：如何计算梯度

```python
import torch

# 创建张量
x = torch.arange(4.0, requires_grad=True)
y = x ** 2 
# 计算梯度
y.sum().backward()  # 对 y 的所有元素求和并进行反向传播

# 查看梯度
print(x.grad)
```

```
tensor([0., 2., 4., 6.])
```

注意：**在默认情况下，PyTorch会累积梯度，我们需要清除之前的值**

```
# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
x.grad.zero_()
```

## 2.5.2. 非标量变量的反向传播

- 当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。 对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。

- **在深度学习中，很少对向量的函数进行求导，更多的是对标量进行求导，因此绝大多数情况是`y.sum().backward()`**

## 2.5.3. 分离计算

- 我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数， 并且只考虑到`x`在`y`被计算后发挥的作用。

  这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值， 但丢弃计算图中如何计算`y`的任何信息。

- 向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理， 而不是`z=x*x*x`关于`x`的偏导数。

```
x.grad.zero_()
y = x * x
u = y.detach() # 从计算图中分离 y
z = u * x

z.sum().backward() # 计算 z.sum() 对 x 的梯度
x.grad == u
```

> ```
> tensor([True, True, True, True])
> ```

> ```
> y.detach()
> ```
>
> - 会返回一个新的张量 `u`，这个张量包含与 `y` 相同的值，但它 **不再与计算图相关联**。
>
> ```
> z.sum().backward()
> ```
>
> - `z.sum()` 计算了 `z` 中所有元素的和。因为 `z` 是由 `u` 和 `x` 相乘得到的，而 `u` 是从计算图中分离出来的，所以在对 `z.sum()` 进行 `.backward()` 操作时，只会计算 `z` 对 `x` 的梯度。

- `z.sum().backward()`和`z.backward()`**关键区别**：
  - `z.sum()` 是对 `z` 所有元素的和进行求导，因此 `z.sum()` 对 `x` 的梯度计算时，实际上是将每个 `z_i` 对 `x_i` 的梯度加和，这导致它可能对 `x` 的每个元素产生影响，而不仅仅是局部的影响。
  - `z` 对 `x` 的梯度是针对单个 `z_i` 计算的局部梯度，而 `z.sum()` 对 `x` 的梯度则是针对所有元素的梯度总和。

## 2.5.4. Python控制流的梯度计算

- 即使构建函数的计算图在Python控制流（例如，条件、循环或任意函数调用）中，我们仍然可以计算得到的变量的梯度。

```python
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c
    
a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()
```

> ```
> while b.norm() < 1000:
> ```
>
> - 这一行是一个 `while` 循环，它将持续执行直到 `b` 的范数（`norm`）大于或等于 1000。`b.norm()` 计算的是 `b` 向量的欧几里得范数（即向量的模长），计算公式为：
>
> $$
> \|b\| = \sqrt{b_1^2 + b_2^2 + \cdots + b_n^2}∥b∥=b12+b22+⋯+bn2
> $$
>
> ```py
> a = torch.randn(size=(), requires_grad=True)
> ```
>
> - **`torch.randn(size=(), requires_grad=True)`**: 这行代码创建了一个大小为 0（标量）的张量 `a`，它是从标准正态分布中随机生成的。
>   - `size=()` 表示生成的是一个标量张量（即形状为 `()` 的张量）。如果你传递的是 `size=(3,)`，那么生成的就是一个包含 3 个元素的 1D 张量。
>   - `requires_grad=True` 表示我们希望跟踪该张量在计算图中的操作，这样我们可以计算关于该张量的梯度。

## 2.5.5. 练习

### 练习1

------

**Q1：为什么计算二阶导数比一阶导数的开销要更大？**

------

计算图在计算一阶导数只用记录一次梯度，而二阶导需要记录两次梯度。记录多次梯度都需要重头计算，对每个一阶导数进行反向传播，再一次求导并更新梯度。并且，二阶导数是在一阶导数的基础上进行的。

### 练习2

------

**Q2：在运行反向传播函数之后，立即再次运行它，看看会发生什么。**

------

```py
import torch

x = torch.arrange(4.,requires_grad=True)
y = x ** 2
y.sum().backward()
print(x.grad)
y.sum().backward()
```

------

```
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
```

- 调用 `y.sum().backward()`，进行反向传播，计算 `x` 上的梯度：

  - 对于 `y = x^2`，梯度是：`dy/dx = 2x`。

  - 因此，`x.grad` 应该是 `2 * x`，即 `[0, 2, 4, 6]`。

- 默认情况下，`backward()` 会在执行反向传播后删除计算图。这意味着第二次调用 `backward()` 时，计算图已经不存在了，所以 PyTorch 会抛出一个错误。

```
y.sum().backward(retain_graph=True)  # 保留计算图
print(x.grad)

y.sum().backward()  # 第二次反向传播
```

- 想要在相同的计算图上执行多次反向传播，可以通过 `retain_graph=True` 来保留计算图。

### 3.`在控制流的例子中，我们计算d关于a的导数，如果将变量a更改为随机向量或矩阵，会发生什么？`

```
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c
    
a = torch.randn(size=(1,3),requires_grad=True)
b = torch.randn(size=(),requires_grad=True)

d = f(a)
d.sum().backward()
e = f(b)
e.sum().backward()

print(a,a.grad,a.grad == d/a)
print(b,b.grad,b.grad == e/b)
```

> ```
> tensor([[-0.0524, -0.0269, -0.8601]], requires_grad=True) tensor([[204800., 204800., 204800.]]) tensor([[True, True, True]])
> tensor(0.3119, requires_grad=True) tensor(4096.) tensor(True)
> ```

### 4.`重新设计一个求控制流梯度的例子，运行并分析结果。`

```py
def f(x,order):
    if order = 1:
        y = torch.sqrt(x)
    else order = 2:
        y = torch.square(x)
    return y

x = torch.randn(size=(),requires_grad=True)
g1 = f(x,order=1)
g1.sum().backward()
print(x,x.grad)

x.grad.zero_()

g2 = f(x,order = 2)
g2.sum().backward()
print(x,x.grad)
```

> ```
> tensor(1.2469, requires_grad=True) tensor(0.4478)
> tensor(1.2469, requires_grad=True) tensor(2.4937)
> ```

> 1. **第一次调用 `f(x, order=1)`**:
>
> ```python
> g1 = f(x, order=1)  # order=1, 计算 sqrt(x)
> g1.sum().backward()
> ```
>
>  `x = 1.2469`,`g1 = torch.sqrt(x)` 的结果为：
>
> $$
> g1 = \sqrt{x} = \sqrt{1.2469} \approx 1.1168
> $$
> 根据链式法则，`g1 = \sqrt{x}` 对 `x` 的导数是：
> $$
> \frac{d}{dx} (\sqrt{x}) = \frac{1}{2\sqrt{x}}
> $$
> 代入 `x = 1.2469` 计算得：
> $$
> \frac{1}{2\sqrt{1.2469}} = \frac{1}{2 \times 1.1168} \approx 0.4478
> $$
> 因此，`x.grad` 的值是 `tensor(0.4478)`。
>
> ------
>
> 1. **第二次调用 `f(x, order=2)`**:
>
> ```python
> x.grad.zero_()  # 清零梯度
> 
> g2 = f(x, order=2)  # order=2, 计算 x^2
> g2.sum().backward()
> ```
>
> 对于 `x = 1.2469`：
> $$
> g2 = x^2 = (1.2469)^2 \approx 1.5537
> $$
> `g2 = x^2` 对 `x` 的导数是：
> $$
> \frac{d}{dx} (x^2) = 2x
> $$
> 代入 `x = 1.2469` 计算得：
> $$
> 2 \times 1.2469 = 2.4937
> $$
> 所以，`x.grad` 的值是 `tensor(2.4937)`。

### 5.`使f (x) = sin(x)，绘制f(x)和df(x)/dx的图像，其中后者不使用f'(x)= cos(x)。`

```
import numpy as np
from matplotlib_inline import backend_inline
from d2l import torch as d2l
import torch

def use_svg_display():
    """使用svg格式在Jupyter中显示绘图"""
    backend_inline.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)): 
    """设置matplotlib的图表大小"""
    use_svg_display()
    d2l.plt.rcParams['figure.figsize'] = figsize

def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
    """设置matplotlib的轴"""
    axes.set_xlabel(xlabel)
    axes.set_ylabel(ylabel)
    axes.set_xscale(xscale)
    axes.set_yscale(yscale)
    axes.set_xlim(xlim)
    axes.set_ylim(ylim)
    if legend:
        axes.legend(legend)
    axes.grid()

def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
         ylim=None, xscale='linear', yscale='linear',
         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
    """绘制数据点"""
    if legend is None:
        legend = []

    set_figsize(figsize)
    axes = axes if axes else d2l.plt.gca()

    # 如果X有一个轴，输出True
    def has_one_axis(X):
        return (hasattr(X, "ndim") and X.ndim == 1 or isinstance(X, list)
                and not hasattr(X[0], "__len__"))

    if has_one_axis(X):
        X = [X]
        
    if Y is None:
        X, Y = [[]] * len(X), X
    elif has_one_axis(Y):
        Y = [Y]
        
    if len(X) != len(Y):
        X = X * len(Y)
        
    axes.cla()
    
    for x, y, fmt in zip(X, Y, fmts):
        if len(x):
            axes.plot(x, y, fmt)
        else:
            axes.plot(y, fmt)
    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)

def function_grad():
    x = torch.linspace(-10,10,300,requires_grad=True)
    y = torch.sin(x)
    y.sum().backward()
    dy_dx = x.grad
    # plot(x,[y,dy_dx],xlabel='x', ylabel='y', legend=[r'$f(x) = \sin(x)$', r"$f'(x) = \cos(x)$ (auto-diff)"])
    plot(x.detach().numpy(), 
         [y.detach().numpy(), dy_dx.detach().numpy()],
         xlabel='x', ylabel='y', 
         legend=[r'$f(x) = \sin(x)$', r"$f'(x) = \cos(x)$"])
    
function_grad()
```

![image-20250105210549484](https://raw.githubusercontent.com/Gainwu/img_cloud/main/img/image-20250105210549484.png)

> 有三种方式实现。

<details>
  <summary>点击时的区域标题：点击查看详细内容</summary>
    ```
    #define A B
    #endif
    void init(void)
    ```
</details>



<details>   
    <summary>折叠代码块可高亮</summary>   
    <pre><code>       
    	System.out.println("虽然可以折叠代码块");      
    	System.out.println("但是代码可高亮");   
    </code></pre> 
</details>


<details>   
    <summary>视频笔记（截图）</summary>   

![image-20250103203831392](https://raw.githubusercontent.com/Gainwu/img_cloud/main/img/image-20250103203831392.png)

![image-20250103203925706](https://raw.githubusercontent.com/Gainwu/img_cloud/main/img/image-20250103203925706.png)
</details>